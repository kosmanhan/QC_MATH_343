---
title: "Practice Assignment 2 MATH 343"
author: "Osman Khan"
output: pdf_document
date: "11:59PM April 12d"
---

This practice assignment is coupled to the theory assignment (the problem numbers align herein) and should be worked on concomitantly. You will write code in places marked "TO-DO" to complete the problems. Most of this will be a pure programming assignment but there are some questions that instead ask you to "write a few sentences" which are not R chunks.

The tools for solving these problems can be found in the class demos located [here](https://github.com/kapelner/QC_MATH_343_Spring_2024/tree/main/demos).

To "hand in" the homework, push this completed file by the due date to your course repo.

NOT REQUIRED: After you're done, you have the option to compile this file into a PDF (use the "knit to PDF" button on the submenu above). These final PDF's look pretty as it includes the output of your code. You can push the PDF as well. It will look nice in your portfolio.

This lab requires the following packages. You should make sure they load before beginning:

```{r}
rm(list = ls())
pacman::p_load(ggplot2, glmnet, survival, lmtest, skimr, MASS, mlbench)
```

## Problem 1: Inference for the linear model using the OLS estimator

Below is a design matrix taken from the boston housing data and a definition of some variables.

```{r}
?MASS::Boston
X = model.matrix(medv ~ ., MASS::Boston)
n = nrow(X)
p_plus_one = ncol(X)
XtX = t(X) %*% X
XtXinv = solve(XtX)
XtXinvXt = XtXinv %*% t(X)
#XtXinvXt = solve(t(X) %*% X) %*% t(X)
H = X %*% XtXinvXt
In_minus_H = diag(n) - H
```

We will now assume betas of all ones and a sigma of 2:

```{r}
betavec = rep(1, p_plus_one)
sigsq = 2^2
```

We will now simulate many response vectors using the core assumption. Remember that the `rnorm` function takes sigma (not sigma-squared) as an argument. Then we'll use the response vectors to compute b, yhat and e. We will collect them all into matrices so we can investigate their behavior later.

```{r}
Nsim = 10000
bs = matrix(NA, nrow = p_plus_one, ncol = Nsim)
yhats = matrix(NA, nrow = n, ncol = Nsim)
es = matrix(NA, nrow = n, ncol = Nsim)
set.seed(1)
for (nsim in 1 : Nsim){
  epsilon_vec = rnorm (n,sqrt(sigsq))
  y = X %*% betavec + epsilon_vec
  yhats[, nsim] = H %*% y
  es[, nsim] = In_minus_H %*% y
  bs[, nsim] = XtXinvXt %*% y
}
```

Let's now make sure the formulas are correct for Yhat. Let's take the 17th observation and standardize its values based on knowledge of the true betas and the formulas from class. We can plot them here:

```{r}
#yhat17=N(vec{x_17},vec{beta}, sigma^2H_17,17)
#
yhat17s_std = (yhats[17, ] - X[17, ]%*% betavec) / sqrt(sigsq * H[17, 17])
ggplot(data.frame(yhat17s_std = yhat17s_std)) + aes(x = yhat17s_std) + geom_histogram()
```

This distribution should look like a standard normal. Confirm that you cannot reject a Kolmogorov-Smirnov test that `yhat17s_std` comes from an iid N(0, 1) DGP:

```{r}
ks.test(yhat17s_std, "pnorm", 0, 1)
```

Repeat this Kolmogorov-Smirnov test for the 7th entry of b.

```{r}
b_7=bs[7,]
ggplot(data.frame(b_7 = b_7)) + aes(x = b_7) + geom_histogram()
b7s_std = (b_7 - X[17, ]%*% betavec) / sqrt(sigsq * H[17, 17])
ggplot(data.frame(b7s_std = b7s_std)) + aes(x = b7s_std) + geom_histogram()
ks.test(b7s_std,"pnorm",0,1)
```

Repeat this Kolmogorov-Smirnov test for the 37th entry of e.

```{r}
e37s_std = (es[17, ] - X[17, ]%*% betavec) / sqrt(sigsq * H[17, 17])
ggplot(data.frame(e37s_std = e37s_std)) + aes(x = e37s_std) + geom_histogram()
ks.test(e37s_std,"pnorm",0,1)
```

Now let's work with just one realization of the errors which gives us one estimate of y, b, yhat and e:

```{r}
b = bs[, 1]
yhat = yhats[, 1]
e = es[, 1]
y = yhat + e
```

At level alpha = 5%, test H_0: beta_7 = 0 by calculating the t-statistic and comparing it to the appropriate critical value of t.

```{r}
s_e = sqrt(sum(e^2) / (n - p_plus_one))
b[7] / (s_e * sqrt(XtXinv[7, 7]))
qt(.975, n - p_plus_one)
```

Create a 95% CI for mu_17, the expected value of the 17th observation in the X matrix.

```{r}
y_hat_star_17=b[7] / (s_e * sqrt(XtXinv[7, 7]))
qt(.975, n - p_plus_one)
s_e
sqrt(XtXinv[7, 7])
c(y_hat_star_17-(qt(.975, n - p_plus_one)*s_e*sqrt(XtXinv[7, 7])),y_hat_star_17+(qt(.975, n - p_plus_one)*s_e*sqrt(XtXinv[7, 7])))
#last page lec 10
```

Create a 95% CI for y_17, the response value for the 17th observation in the X matrix.

```{r}
#TO-DO
#page 3 lec 11
```

Run the omnibus test at level alpha = 5% by calculating the quantities from scratch and comparing to the appropriate critical F value.
 
```{r}
#TO-DO
#msr, mse, calculate f
```

Run the multiple effect test for H_0: beta_1 = beta_2 = beta_3 = 0 at level alpha = 5% by calculating the quantities from scratch and comparing to the appropriate critical F value.

```{r}
#TO-DO
```

Compute the maximum likelihood estimator for sigsq.

```{r}
#TO-DO
```

## Problem 2: Ridge and Lasso predictions

We'll use the data setup from class: the boston housing data with another 1000 garbage features tacked on and then all features standardized:

```{r}
rm(list = ls())
p_extra = 1000

set.seed(1)
y = MASS::Boston$medv
X = model.matrix(medv ~ ., MASS::Boston)
X = cbind(X, matrix(rnorm(nrow(X) * p_extra), ncol = p_extra))
colnames(X) = c("(Intercept)", colnames(MASS::Boston)[1:13], paste0("junk_", 1 : p_extra))

#now we standardize the columns
X = apply(X, 2, function(x_dot_j){(x_dot_j - mean(x_dot_j)) / sd(x_dot_j)})
X[, 1] = 1 #reset the intercept
```

We will now split the data into training (with 400 observations) and test:

```{r}
train_idx = sample(1 : nrow(X), 400)
test_idx = setdiff(1 : nrow(X), train_idx)
Xtrain = X[train_idx, ]
ytrain = y[train_idx]
Xtest =  X[test_idx, ]
ytest =  y[test_idx]
```

In class we fit many ridge models and many lasso models using arbitrary values of lambda. Here we will use the model selection technique from 342W implementing inner K-fold CV but not the outer K-fold CV. We can use the `cv.glmnet` function to do this. You can use its default lambda grid search. Run both ridge and lasso. Report the optimal lambda values for ridge and lasso.

```{r}
#alpha 1= lasso, alpha 0 = ridge, alpha .5 = lasso-ridge mixign
pacman::p_load(glmnet)
cv.glmnet(X, y, alpha = 0)# #ridge
cv.glmnet(X, y, alpha = 1) #lasso
ridge_mod = cv.glmnet(X, y, alpha = 0)
lasso_mod = cv.glmnet(X, y, alpha = 1)
ridge_min=ridge_mod$lambda.min
lasso_min=lasso_mod$lambda.min
```

Now fit both the ridge and lasso models using their respective optimal values of lambda.

```{r}
ridge_mod = glmnet(X, y, alpha = 0, lambda = ridge_min)
lasso_mod = glmnet(X, y, alpha = 1, lambda = lasso_mod$lambda.min)
```

For the lasso model, which features did it select?

```{r}
colnames(X)[which(lasso_mod$beta != 0)]
```

Now predict on the test set and calculate oosRMSE. Who wins?

```{r}
#TO-DO
```


# Problem 3: Robust regression methods

Let's use 1000 rows of the diamonds dataset for this exercise. We'll convert the ordinal factors to nominal to make the feature dummy names more readable.

```{r}
rm(list = ls())
diamonds = ggplot2::diamonds
?diamonds
diamonds$cut =      factor(diamonds$cut, ordered = FALSE)      #convert to nominal
diamonds$color =    factor(diamonds$color, ordered = FALSE)    #convert to nominal
diamonds$clarity =  factor(diamonds$clarity, ordered = FALSE)  #convert to nominal

set.seed(1)
idx = sample(1 : nrow(diamonds), 1000)
X = model.matrix(price ~ ., diamonds[idx, ])
y = diamonds$price[idx]
rm(list = setdiff(ls(), c("X", "y")))
```

Fit a linear model on all features and report the p-value for the test of H_0: beta_j = 0 where j is the index of the `depth` feature.

```{r}
mod =(lm(y ~ 0 + X))
summary(mod)
b = coef(mod)
s_e = summary(mod)$simga

```

Now assume nothing is known about the error DGP except that they are independent.

Report an asymptotically valid p-value for the test of H_0: beta_j = 0 where j is the index of the `depth` feature.

```{r}
#TO-DO
#bootstrapping, 
#want speed? Rcpp it
```

Now assume the errors are mean-centered and homoskedastic. 

Report an asymptotically valid p-value for the test of H_0: beta_j = 0 where j is the index of the `depth` feature.

```{r}
#TO-DO
#calculate t-stat from before, but it's asymptotically z.
```

Report an asymptotically valid p-value for the test of H_0: beta_j = 0 and beta_k = 0 where j is the index of the `depth` feature and k is the index of the `table` feature.

```{r}
#TO-DO
#lec 15 pg 3
names(b)
s_set = c(20, 21)
b = coef(mod)
s_e = summary(mod)$simga
XtXinv = solve(t(X) %*% X)

1 / s_e^2 * t(b[s_set]) %*% solve(XtXin[s_set, s_set]) %*% b[s_set]
qchisq(.95, 2)
```

Now assume the errors are mean-centered and heteroskedastic. This is the scenario where you employ the Huber-White estimator.

Report an asymptotically valid p-value for the test of H_0: beta_j = 0 where j is the index of the `depth` feature.

```{r}
#TO-DO
```

Report an asymptotically valid p-value for the test of H_0: beta_j = 0 and beta_k = 0 where j is the index of the `depth` feature and k is the index of the `table` feature.

```{r}
#TO-DO
```

# Problem 4a: Inference for Bernoulli Response Models

We load up the Glass dataset below. The goal is to predict and understand the effects of features on whether or not the glass is of type 1.

```{r}
rm(list = ls())
data(Glass)
glass = na.omit(Glass)
glass$Type = ifelse(glass$Type == 1, 1, 0)
```

Fit a probit regression using all features and report p-values for H_0: beta_j = 0 for all features. Using the `glm` function with `family = binomial(link = "probit")`.

```{r}
#TO-DO
```

Run the omnibus test at level alpha=5% to see if any of these features are useful in predicting the probability of Type=1.

```{r}
#TO-DO
```


Predict the probability of glass being of type 1 if the sample had average amounts of all features.

```{r}
x_vec_avg = data.frame(t(apply(glass, 2, mean)))
#TO-DO
```

Add quadratic terms to all the features and fit a new model. Check if these additional features are justified at level alpha=5%.

```{r}
#TO-DO
```

# Problem 4b: Inference for Poisson Count Response Model

We load up the insurance dataset below. The goal is to predict and understand the effects of features on number of car insurance claims (the `Claims` column).

```{r}
rm(list = ls())
insur = MASS::Insurance
insur$Group = factor(insur$Group, ordered = FALSE)
insur$Age = factor(insur$Age, ordered = FALSE)
```

Fit a poisson count model (AKA "Poisson regression") to the data and report p-values for H_0: beta_j = 0 for all features. Using the `glm` function with `family="poisson"` defaults to the log link.

```{r}
#TO-DO
```

Predict the number of claims (to the nearest claim) for a someone who lives in a major city, who's age 26, has a 1.8L engine car and has only one policy.

```{r}
#TO-DO
```

Now fit a Poisson count model that includes the interaction of Age and Holders. Test whether the addition of these interactions is warranted at level alpha=5%.

```{r}
#TO-DO
```

# Problem 4c: Inference for Negative Binomial Count Response Model

Fit a Negative Binomial count model (AKA "negative binomial regression") to the data and report p-values for H_0: beta_j = 0 for all features. To do this use the `glm.nb` which defaults to the log link.

```{r}
#TO-DO
```

Predict the number of claims (to the nearest claim) for a someone who lives in a major city, who's age 26, has a 1.8L engine car and has only one policy.

```{r}
#TO-DO
```

Now fit a Negative Binomial count model that includes the interaction of Age and Holders. Test whether the addition of these interactions is warranted at level alpha=5%.

```{r}
#TO-DO
```


Were there any substantive differences between the inference of prediction you found between the Poisson and Negative Binomial models?

#TO-DO


# Problem 4d: Inference for the Weibull Survival Model

Let's load up data from a trial of usrodeoxycholic acid.

```{r}
rm(list = ls())
udca2 = na.omit(survival::udca2)
?udca2
survival_time = udca2$futime
uncensored_dummy = udca2$status
udca2$id = NULL
udca2$status = NULL
udca2$futime = NULL
udca2$endpoint = NULL
```

We now create a surv object and print out the first 20 entries.

```{r}
surv_obj = Surv(survival_time, uncensored_dummy)
rm(survival_time, uncensored_dummy)
head(surv_obj, 20)
```

What do the "+" signs mean in the above print out?

Those datapoint were censored.

Fit a Weibull regression model to all features and report p-values for H_0: beta_j = 0 for all features.

```{r}
#TO-DO
```

Predict the survival time for a subject with the UDCA treatment (i.e. trt = 1), stage = 1, bili = 1.5 and riskscore = 4.0.

```{r}
#TO-DO
```

Run the omnibus test at alpha=5%.

```{r}
#TO-DO
```

Run the test to see if the variables stage, bili and riskscore are important in predicting survival at alpha=5%.

```{r}
#TO-DO
```

